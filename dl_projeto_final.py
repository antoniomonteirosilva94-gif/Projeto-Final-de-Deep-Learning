# -*- coding: utf-8 -*-
"""Cópia de Exper de projeto_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KamHgE7SZsNwcbxJGrXDK32ddJTrqnjV

# Estrutura do Projeto
Você usará um único notebook .ipynb com as seções abaixo.

**Configuração e Setup Inicial**
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import torch

!pip install torch numpy matplotlib pyyaml

# %cd /content/
PROJECT_ROOT = "project_final"
if os.path.isdir(PROJECT_ROOT):
    import shutil
    shutil.rmtree(PROJECT_ROOT)
os.makedirs(f"{PROJECT_ROOT}/src/models", exist_ok=True)
os.makedirs(f"{PROJECT_ROOT}/src/utils", exist_ok=True)
os.makedirs(f"{PROJECT_ROOT}/results/figures", exist_ok=True)

# Commented out IPython magic to ensure Python compatibility.
# %cd {PROJECT_ROOT}

"""**README.md**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile README.md
# 
# # Projeto Final — Deep Learning: Aplicação de Redes Neurais Informadas pela Física (PINNs) na Solução da Equação de Wheeler–DeWitt em um Modelo quântico-cosmológico
# 
# **Autor:** António Monteiro
# **Matrícula:** [DO2520057]
# **Curso:** Doutorado em Modelagem Computacional, UERJ/IPRJ (Nova Friburgo)
# 
# ---
# 
# ## Descrião
# 
# Este projeto implementa **Redes Neurais Informadas pela Física (PINNs)** para resolver o problema de autovalores associado à **equação de Wheeler–DeWitt estacionária** de um modelo cosmológico quântico, comparando os resultados com os valores de referência obtidos numericamente (método espectral) na **Tabela 11** da dissertação.
# 
# A ideia central é:
# 
# > Em vez de resolver o problema de autovalores com um método numérico tradicional, deixar o **PINN aprender simultaneamente as autofunções \(\eta_n(a)\) e os autovalores \(E_n\)**, apenas impondo a equação diferencial, as condições de contorno e condições de ortogonalidade/ordenação.
# 
# ---
# 
# ## 1. Modelo físico
# 
# Consideramos a equação de Wheeler–DeWitt (dependente do tempo) do modelo cosmológico-quântico adotado na dissertação. Para soluções estacionárias do tipo
# 
# \[
# \Psi(a,\tau) = e^{-iE\tau}\,\eta(a),
# \]
# 
# obtemos o problema de autovalor:
# 
# \[
# -\frac{d^2\eta}{da^2} + V(a)\,\eta(a) = 12E\,\eta(a), \quad a\in[0,L],
# \]
# 
# com condições de contorno de Dirichlet homogêneas:
# 
# \[
# \eta(0) = \eta(L) = 0.
# \]
# 
# O potencial efetivo é dado por
# 
# \[
# V(a) = 36a^2 + 12|\Lambda|a^4 + 12 a V_0 \,\text{sech}^2(a),
# \]
# 
# onde:
# 
# - \(\Lambda\) é a constante cosmológica (aqui, \(\Lambda = -0{,}001\)),
# - \(L = 3\) é o tamanho do domínio em \(a\),
# - \(V_0\) controla a profundidade do poço (valores típicos: \(-1, -5, -10, -15\)).
# 
# Os **autovalores de referência** \(E_n\) para os primeiros 15 níveis de energia são aqueles apresentados na **Tabela 11 da dissertação**, para \(\Lambda=-0{,}001\) e \(L=3\).
# 
# ---
# 
# ## 2. Objetivo do projeto
# 
# - Utilizar **PINNs** para resolver o problema de autovalores da equação de Wheeler–DeWitt estacionária.
# - Deixar a rede neural:
#   - aprender **sozinha** as autofunções \(\eta_n(a)\),
#   - e **aprender sozinha** os autovalores \(E_n\), que são tratados como **parâmetros treináveis**.
# - Comparar os autovalores obtidos pela PINN com os valores da **Tabela 11**:
#   - para diferentes valores de \(V_0\),
#   - e para diferentes números de níveis considerados (\(5, 10, 15\)).
# 
# ---
# 
# ## 3. Estrutura do projeto
# 
# ```text
# .
# ├── config.yaml                 # Configurações do modelo, física e treino
# ├── requirements.txt            # Dependências principais do projeto
# ├── src
# │   ├── models
# │   │   └── dnn.py             # Implementação do PINN (SpectrumSolver)
# │   └── utils
# │       └── helpers.py         # Funções auxiliares (potencial, gráficos, etc.)
# ├── results
# │   ├── best_pinns_model.pt    # Melhor modelo salvo durante o treino
# │   ├── metrics_pinns.json     # Métricas numéricas (autovalores e erros)
# │   └── figures
# │       ├── loss_curve.png     # Curva da função de loss ao longo do treino
# │       └── autofunctions_*.png# Gráficos das autofunções
# └── main.py (opcional)         # Script para chamar o treino/avaliação
# 
# 
# ## Referências
# 
# Monteiro, António.
# Cosmologia quântica computacional : aplicação do método
# espectral de Galerkin no estudo da dinâmica do universo primitivo
# descrito por radiação, constante cosmológica negativa e um
# potencial de Pöschl-Teller / António Monteiro. - 2025.
# 74 f. : il.

"""**requirements.txt**"""

import yaml
import os

# Cria requirements.txt
requirements_list = ["torch", "numpy", "matplotlib", "pyyaml", "pandas"]
with open("requirements.txt", "w") as f:
    f.write("\n".join(requirements_list) + "\n")

"""**config.yaml**"""

import yaml
import os

# Cria requirements.txt (opcional)
requirements_list = ["torch", "numpy", "matplotlib", "pyyaml", "pandas"]
with open("requirements.txt", "w") as f:
    f.write("\n".join(requirements_list) + "\n")

# ------------------------------
# config.yaml (15 níveis, Tabela 11 – Λ = -0.001, L = 3)
# ------------------------------
config_data = {
    "model": {
        "num_states": 15,                # 15 primeiros níveis de energia
        "hidden_layers": [128, 128, 64, 32]
    },
    "physics": {
        "Lambda": -0.001,
        "domain_L": 3.0,

        # Escolha aqui o V0 que deseja estudar (Tabela 11)
        # Opções: -1.0, -5.0, -10.0, -15.0
        "V0": -1.0,

        # Tabela 11 – Espectro de energia (15 níveis)
        # Aqui, consideramos Λ = −0.001 e L = 3.0
        # Fonte: dissertação, Tabela 11  :contentReference[oaicite:2]{index=2}
        "E_table11": {
            "-1.0": [
                1.1524035661087500,
                3.1330720784237460,
                5.1530449047380950,
                7.1807999922368520,
                9.2082565288106050,
                11.2333299550090900,
                13.2557251190887500,
                15.2756902548776400,
                17.2936609168152200,
                19.3107072998228800,
                21.3324085979928600,
                23.3824732975619600,
                25.5179840364431700,
                27.8094868770646700,
                30.3002460864610400
            ],
            "-5.0": [
                -0.2957769212457393,
                1.6749821180671290,
                3.7644848933642370,
                5.8902169616320510,
                8.0177393097141390,
                10.1351169760011000,
                12.2393088557759500,
                14.3306177415335100,
                16.4104878504059800,
                18.4809248714588000,
                20.5465949721401900,
                22.6254069381264900,
                24.7672277103157000,
                27.0465166326122500,
                29.5186895135900400
            ],
            "-10.0": [
                -2.2120270774085930,
                -0.1400966315254728,
                2.0365526416190890,
                4.2698274279315670,
                6.5129809934991150,
                8.7422916250953960,
                10.9489505358632600,
                13.1314339413944500,
                15.2913344361363000,
                17.4314604191034900,
                19.5561719190862500,
                21.6781272398591900,
                23.8368710588819000,
                26.1061165450413400,
                28.5554622335917500
            ],
            "-15.0": [
                -4.2094442067553760,
                -1.9718754485697060,
                0.3101718010128308,
                2.6434415908670510,
                4.9930283161244430,
                7.3293290902373810,
                9.6373558554571590,
                11.9122650870953700,
                14.1545825648532700,
                16.3672684893056400,
                18.5547225043604900,
                20.7263424269666100,
                22.9118772998944000,
                25.1787289371346800,
                27.6071418433966600
            ]
        }
    },
    "training": {
        "learning_rate": 0.0003,
        "epochs": 20000,
        "colocation_points": 2000,   # pode ajustar
        "seed": 42,
        "weight_ortogonalidade": 500.0,
        "weight_ordenação": 50.0
    },
    "paths": {
        "checkpoint": "results/best_pinns_model.pt",
        "metrics": "results/metrics_pinns.json"
    }
}

os.makedirs("results", exist_ok=True)
config_path = "config.yaml"
with open(config_path, "w") as f:
    yaml.dump(config_data, f, default_flow_style=False)

print("config.yaml criado/atualizado com sucesso!")

"""**src/utils/helpers.py**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile src/utils/helpers.py
# import os
# import json
# import random
# 
# import numpy as np
# import torch
# import matplotlib.pyplot as plt
# import yaml
# 
# 
# def set_seed(seed: int):
#     """Fixa sementes para garantir reprodutibilidade."""
#     random.seed(seed)
#     np.random.seed(seed)
#     torch.manual_seed(seed)
#     if torch.cuda.is_available():
#         torch.cuda.manual_seed_all(seed)
# 
# 
# def ensure_dir(path: str):
#     """Garante que o diretório do arquivo exista."""
#     directory = os.path.dirname(path)
#     if directory and not os.path.exists(directory):
#         os.makedirs(directory, exist_ok=True)
# 
# 
# def save_metrics(metrics: dict, path: str = "results/metrics_pinns.json"):
#     """Salva métricas em JSON."""
#     ensure_dir(path)
#     with open(path, "w") as f:
#         json.dump(metrics, f, indent=4)
#     print(f"Métricas salvas em: {path}")
# 
# 
# def load_config(path: str = "config.yaml") -> dict:
#     """Carrega o dicionário de configuração."""
#     with open(path, "r") as f:
#         return yaml.safe_load(f)
# 
# 
# def pot_V(a: torch.Tensor, Lambda: float, V0: float) -> torch.Tensor:
#     r"""
#     Potencial efetivo do modelo quântico-cosmológico:
# 
#         V(a) = 36 a^2 + 12 |Λ| a^4 + 12 a V0 sech^2(a)
# 
#     Conforme equações (62)–(63) da dissertação. :contentReference[oaicite:3]{index=3}
#     """
#     Lambda_abs = abs(Lambda)
#     sech_sq = (1.0 / torch.cosh(a)) ** 2
#     V_eff = 36.0 * a**2 + 12.0 * Lambda_abs * a**4 + 12.0 * a * V0 * sech_sq
#     return V_eff
# 
# 
# def plot_loss(losses: dict, path: str = "results/figures/loss_curve.png"):
#     """Plota a curva de loss do treinamento."""
#     ensure_dir(path)
#     fig, ax = plt.subplots(figsize=(8, 6))
#     ax.plot(losses["total"], label="Total Loss")
#     ax.plot(losses["edp"], label="EDP Loss")
#     ax.plot(losses["cc"], label="CC Loss")
#     ax.plot(losses["orto"], label="Ortogonalidade")
#     ax.plot(losses["ordenação"], label="Ordenação")
# 
#     ax.set_xlabel("Época")
#     ax.set_ylabel("Loss")
#     ax.set_yscale("log")
#     ax.grid(True, linestyle="--", alpha=0.3)
#     ax.legend()
#     plt.tight_layout()
#     plt.savefig(path, dpi=200)
#     plt.close(fig)
#     print(f"Curva de loss salva em: {path}")
# 
# 
# def plot_autofunctions(
#     eta_all: torch.Tensor,
#     a: torch.Tensor,
#     L: float,
#     num_states_to_plot: int = 3,
#     path: str = "results/figures/autofunctions.png",
# ):
#     """
#     Plota as autofunções η_n(a) (normalizadas apenas em amplitude máxima)
#     para inspeção qualitativa.
#     """
#     ensure_dir(path)
# 
#     a_np = a.detach().cpu().numpy().flatten()
#     eta_np = eta_all.detach().cpu().numpy()
# 
#     fig, ax = plt.subplots(figsize=(8, 6))
#     for n in range(num_states_to_plot):
#         eta_n = eta_np[:, n]
#         max_abs = np.max(np.abs(eta_n)) + 1e-12
#         eta_n_norm = eta_n / max_abs
#         ax.plot(a_np, eta_n_norm, label=f"$\\eta_{n+1}(a)$ (normalizada)")
# 
#     ax.set_xlabel("a")
#     ax.set_ylabel("$\\eta_n(a)$ (escala arbitrária)")
#     ax.set_title(f"Primeiros {num_states_to_plot} autoestados – domínio [0, {L}]")
#     ax.grid(True, linestyle="--", alpha=0.3)
#     ax.legend()
#     plt.tight_layout()
#     plt.savefig(path, dpi=200)
#     plt.close(fig)
#     print(f"Autofunções salvas em: {path}")

"""**src/models/dnn.py**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile src/models/dnn.py
# import torch
# import torch.nn as nn
# from torch.autograd import grad
# 
# from src.utils.helpers import pot_V
# 
# 
# class SpectrumSolver(nn.Module):
#     """
#     PINN para resolver o problema de autovalores estacionários
#     da equação de Wheeler–DeWitt com N autoestados.
# 
#         -η''(a) + V(a) η(a) = 12 E η(a),  a ∈ (0, L)
#         η(0) = η(L) = 0
# 
#     A saída da rede é (N estados) em paralelo.
#     """
# 
#     def __init__(
#         self,
#         L: float,
#         num_states: int = 3,
#         E_init_values: torch.Tensor | None = None,
#         hidden_layers: list[int] = [256, 128, 64, 32],
#     ):
#         super().__init__()
# 
#         self.L = L
#         self.num_states = num_states
# 
#         # Inicialização dos autovalores E_n (parâmetros treináveis)
#         if E_init_values is None:
#             # algo espalhado entre 1 e 30 (ajuste fino se quiser)
#             E_init_values = torch.linspace(1.0, 30.0, steps=num_states)
#         E_init_values = E_init_values.flatten().float()
# 
#         self.E_n = nn.Parameter(E_init_values.clone().detach())
# 
#         # MLP densa
#         layers = []
#         in_features = 1
#         for h in hidden_layers:
#             layers.append(nn.Linear(in_features, h))
#             layers.append(nn.Tanh())
#             in_features = h
#         layers.append(nn.Linear(in_features, num_states))
# 
#         self.network = nn.Sequential(*layers)
# 
#     def forward_raw(self, a: torch.Tensor) -> torch.Tensor:
#         """Saída bruta da MLP (sem impor CC)."""
#         return self.network(a)
# 
#     def forward(self, a: torch.Tensor) -> torch.Tensor:
#         """
#         Aplica o truque a(L−a) para impor Dirichlet homogênea:
#             η_n(a) = a (L − a) η_n^raw(a)
#         """
#         a_scaled = a * (self.L - a)
#         eta_raw = self.forward_raw(a)
#         return a_scaled * eta_raw
# 
#     def compute_residual(
#         self, a: torch.Tensor, Lambda: float, V0: float
#     ) -> tuple[torch.Tensor, torch.Tensor]:
#         """
#         Calcula o resíduo da EDP para todos os estados simultaneamente:
# 
#             R_n(a) = -η_n''(a) + V(a) η_n(a) - 12 E_n η_n(a)
# 
#         Retorna:
#             R_all: (N_points, num_states)
#             eta_all: (N_points, num_states)
#         """
#         a = a.clone().detach().requires_grad_(True)
# 
#         eta_all = self.forward(a)  # (N_points, num_states)
#         V_eff = pot_V(a, Lambda, V0)  # (N_points, 1) broadcasting
# 
#         R_list = []
#         num_states = self.num_states
# 
#         for n in range(num_states):
#             eta_n = eta_all[:, n : n + 1]  # (N_points, 1)
# 
#             # primeira derivada
#             d_eta_da = grad(
#                 eta_n,
#                 a,
#                 torch.ones_like(eta_n),
#                 create_graph=True,
#                 retain_graph=True,
#             )[0]
# 
#             # segunda derivada
#             d2_eta_da2 = grad(
#                 d_eta_da,
#                 a,
#                 torch.ones_like(d_eta_da),
#                 create_graph=True,
#                 retain_graph=True,
#             )[0]
# 
#             E_n = self.E_n[n]
#             R_n = -d2_eta_da2 + V_eff * eta_n - 12.0 * E_n * eta_n
#             R_list.append(R_n)
# 
#         R_all = torch.cat(R_list, dim=1)
#         return R_all, eta_all

"""**Treinamento do Modelo PINN (train_dl.py)**"""

import os
import json
import math
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from src.utils.helpers import (
    set_seed,
    load_config,
    pot_V,
    plot_loss,
    plot_autofunctions,
    save_metrics,
)
from src.models.dnn import SpectrumSolver


def train_model_pinns_spectrum():
    # ---- Setup básico ----
    config = load_config()
    set_seed(config["training"]["seed"])

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Usando dispositivo: {device}")

    Lambda = config["physics"]["Lambda"]
    V0 = config["physics"]["V0"]
    L = config["physics"]["domain_L"]
    num_states = config["model"]["num_states"]

    # Lê Tabela 11 correspondente ao V0 escolhido
    E_table11 = config["physics"]["E_table11"]
    E_baseline = np.array(
        E_table11[str(float(V0))][:num_states], dtype=np.float32
    )

    weight_orto = config["training"]["weight_ortogonalidade"]
    weight_ord = config["training"]["weight_ordenação"]
    col_points = config["training"]["colocation_points"]
    epochs = config["training"]["epochs"]
    lr = config["training"]["learning_rate"]

    # ---- Modelo ----
    E_init_values = torch.linspace(1.0, 30.0, steps=num_states)
    model = SpectrumSolver(
        L=L,
        num_states=num_states,
        E_init_values=E_init_values,
        hidden_layers=config["model"]["hidden_layers"],
    ).to(device)

    # Pontos de colocação internos (amostragem uniforme)
    a_collocation = torch.rand(col_points, 1, device=device) * L
    a_collocation.requires_grad_(True)

    # Pontos de fronteira
    a_boundary_0 = torch.tensor([[0.0]], dtype=torch.float32, device=device)
    a_boundary_L = torch.tensor([[L]], dtype=torch.float32, device=device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    best_loss_edp = float("inf")
    losses_history = {
        "total": [],
        "edp": [],
        "cc": [],
        "orto": [],
        "ordenação": [],
    }

    print("\nIniciando Treinamento (Gap mínimo forçado de {1.0}).")
    energy_gap_min = 1.0  # mantém os níveis minimamente separados

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()

        # -------------------------
        # A. Loss EDP (resíduo)
        # -------------------------
        R_all, eta_all = model.compute_residual(a_collocation, Lambda, V0)
        loss_edp = criterion(R_all, torch.zeros_like(R_all))

        # -------------------------
        # B. Loss Condições de Contorno
        # -------------------------
        eta_0_all = model(a_boundary_0)
        eta_L_all = model(a_boundary_L)
        loss_cc = criterion(eta_0_all, torch.zeros_like(eta_0_all)) + criterion(
            eta_L_all, torch.zeros_like(eta_L_all)
        )

        # -------------------------
        # C. Loss Ortogonalidade
        # -------------------------
        # Normaliza por coluna (cada estado)
        eta_normalized = eta_all / (
            torch.linalg.norm(eta_all, dim=0, keepdim=True) + 1e-8
        )
        overlap_matrix = (eta_normalized.T @ eta_normalized) / col_points
        target_matrix = torch.eye(num_states, device=device)
        mask = ~target_matrix.bool()
        loss_orto = criterion(overlap_matrix[mask], target_matrix[mask])

        # -------------------------
        # D. Loss Ordenação (E_{n+1} > E_n + gap_min)
        # -------------------------
        loss_ord = 0.0
        for i in range(num_states - 1):
            diff = model.E_n[i + 1] - model.E_n[i]
            loss_ord = loss_ord + torch.relu(energy_gap_min - diff)

        # Loss total
        loss = loss_edp + loss_cc + weight_orto * loss_orto + weight_ord * loss_ord

        loss.backward()
        optimizer.step()

        # Histórico
        losses_history["total"].append(loss.item())
        losses_history["edp"].append(loss_edp.item())
        losses_history["cc"].append(loss_cc.item())
        losses_history["orto"].append(loss_orto.item())
        losses_history["ordenação"].append(loss_ord.item())

        # Salva melhor estado com base na loss da EDP
        if loss_edp.item() < best_loss_edp:
            best_loss_edp = loss_edp.item()
            ckpt_path = config["paths"]["checkpoint"]
            os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)
            torch.save(model.state_dict(), ckpt_path)

        # Logs periódicos
        if (epoch + 1) % 2000 == 0 or epoch == 0:
            with torch.no_grad():
                E_curr = model.E_n.detach().cpu().numpy().flatten()
            print(
                f"Ep {epoch+1:5d} | "
                f"Loss: {loss.item():.2e} | Loss_EDP: {loss_edp.item():.2e}"
            )
            print("  E_n (atual):", np.round(E_curr, 6))

    # -------------------------
    # Pós-treino: análise final
    # -------------------------
    # Carrega o melhor modelo
    model.load_state_dict(torch.load(config["paths"]["checkpoint"], map_location=device))
    model.eval()

    # Pontos para plot
    a_plot = torch.linspace(0.0, L, 500, device=device).view(-1, 1)
    with torch.no_grad():
        eta_plot = model(a_plot)

    E_final = model.E_n.detach().cpu().numpy().flatten()
    E_error = np.abs(E_final - E_baseline)

    results = {
        "E_DL_Final": E_final.tolist(),
        "E_MSG_Baseline": E_baseline.tolist(),
        "Absolute_Error": E_error.tolist(),
        "Final_EDP_Loss": best_loss_edp,
    }

    # Salva métricas + figuras
    save_metrics(results, config["paths"]["metrics"])

    os.makedirs("results/figures", exist_ok=True)
    plot_loss(losses_history, path="results/figures/loss_curve.png")

    # Plota só os 3 primeiros autoestados (mas os 15 foram treinados)
    plot_autofunctions(
        eta_plot,
        a_plot,
        L,
        num_states_to_plot=3,
        path=f"results/figures/autofunctions_V0{int(V0)}.png",
    )

    print("\nTreinamento concluído.")
    print("Autovalores DL (PINN):", E_final)
    print("Autovalores MSG (Tabela 11):", E_baseline)
    print("Erro absoluto médio:", E_error.mean())

"""**Pacote de Onda**"""

def plot_wavepacket_from_pinn(
    tau_values=[0, 1, 2, 3, 4, 5],
    num_superposed_states=3,
    path="results/figures/wavepacket_V0-1.png"
):
    """
    Constrói e plota um pacote de onda a partir da superposição
    dos primeiros autoestados aprendidos pelo PINN:

        Ψ(a,τ) = sum_{n=1}^M C_n η_n(a) e^{-i E_n τ},

    com:
      - η_n(a) normalizados em L2 no intervalo [0, L];
      - Ψ(a,τ) normalizado em τ = 0 (norma ≈ 1).

    Se len(tau_values) == 1 -> um único gráfico.
    Se len(tau_values) > 1  -> grid de subplots.
    """
    import math
    import os
    import matplotlib.pyplot as plt
    import torch

    from src.utils.helpers import load_config
    from src.models.dnn import SpectrumSolver

    config = load_config()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    Lambda = config["physics"]["Lambda"]
    V0 = config["physics"]["V0"]
    L = config["physics"]["domain_L"]
    num_states = config["model"]["num_states"]

    # Reconstrói o modelo exatamente como no treino e carrega o melhor checkpoint
    E_init_values = torch.linspace(1.0, 30.0, steps=num_states)
    model = SpectrumSolver(
        L=L,
        num_states=num_states,
        E_init_values=E_init_values,
        hidden_layers=config["model"]["hidden_layers"],
    ).to(device)

    ckpt_path = config["paths"]["checkpoint"]
    model.load_state_dict(torch.load(ckpt_path, map_location=device))
    model.eval()

    # Pontos em a
    N_points = 500
    a_plot = torch.linspace(0.0, L, N_points, device=device).view(-1, 1)
    dx = L / (N_points - 1)

    with torch.no_grad():
        eta_all = model(a_plot)   # (N_points, num_states)

    # --- Normalização dos autoestados em L2([0,L]) ---
    M = min(num_superposed_states, num_states)
    eta_sub = eta_all[:, :M]              # (N_points, M)

    # norma^2_n ≈ ∫ |eta_n|^2 da ≈ sum_k |eta_n(a_k)|^2 * dx
    norms_sq = torch.sum(eta_sub**2, dim=0) * dx
    norms = torch.sqrt(norms_sq + 1e-12)
    eta_sub = eta_sub / norms             # cada coluna agora tem norma ≈ 1

    E_sub = model.E_n[:M]

    # Coeficientes da superposição (iguais e normalizados)
    C = torch.ones(M, dtype=torch.cfloat, device=device) / math.sqrt(M)
    a_np = a_plot.detach().cpu().numpy().flatten()
    n_tau = len(tau_values)

    # --- calcula norma do pacote em τ = 0 ---
    psi0 = torch.zeros_like(eta_sub[:, 0], dtype=torch.cfloat, device=device)
    for n in range(M):
        psi0 = psi0 + C[n] * eta_sub[:, n]   # fase = 1 em τ=0
    norm_psi0_sq = torch.sum((psi0.conj() * psi0).real) * dx
    norm_psi0 = torch.sqrt(norm_psi0_sq + 1e-12)

    # ------------------------------------------------------------------
    # Caso 1: apenas um τ -> gráfico único
    # ------------------------------------------------------------------
    if n_tau == 1:
        tau = tau_values[0]

        psi = torch.zeros_like(eta_sub[:, 0], dtype=torch.cfloat, device=device)
        for n in range(M):
            phase = torch.exp(-1j * E_sub[n] * tau)
            psi = psi + C[n] * eta_sub[:, n] * phase

        psi = psi / norm_psi0
        prob = (psi.conj() * psi).real
        prob_np = prob.detach().cpu().numpy()

        fig, ax = plt.subplots(figsize=(8, 4))
        ax.plot(a_np, prob_np)
        ax.set_xlabel("$a$")
        ax.set_ylabel(r"$|\Psi(a,\tau)|^2$")
        ax.set_title(
            r"Pacote de onda para $\tau = "
            + f"{tau}$ ("
            + rf"V$_0={V0}$, $\Lambda={Lambda}$)"
        )
        ax.grid(True, linestyle="--", alpha=0.3)

    # ------------------------------------------------------------------
    # Caso 2: vários τ -> grid de subplots
    # ------------------------------------------------------------------
    else:
        fig, axes = plt.subplots(2, 3, figsize=(12, 6), sharex=True, sharey=True)
        axes = axes.flatten()

        for k, tau in enumerate(tau_values):
            psi = torch.zeros_like(eta_sub[:, 0], dtype=torch.cfloat, device=device)
            for n in range(M):
                phase = torch.exp(-1j * E_sub[n] * tau)
                psi = psi + C[n] * eta_sub[:, n] * phase

            psi = psi / norm_psi0
            prob = (psi.conj() * psi).real
            prob_np = prob.detach().cpu().numpy()

            ax = axes[k]
            ax.plot(a_np, prob_np)
            ax.set_title(rf"$\tau = {tau}$")
            ax.grid(True, linestyle="--", alpha=0.3)

        for ax in axes:
            ax.set_xlabel("$a$")
        axes[0].set_ylabel(r"$|\Psi(a,\tau)|^2$")

        fig.suptitle(
            r"Pacote de onda: superposição dos 3 primeiros autoestados "
            + rf"(V$_0={V0}$, $\Lambda={Lambda}$)"
        )

    os.makedirs(os.path.dirname(path), exist_ok=True)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close(fig)

    print(f"Figura do pacote de onda salva em: {path}")

"""**Avaliação e Comparação (eval_dl.py)**"""

import json
import numpy as np
import pandas as pd

from src.utils.helpers import load_config


def evaluate_pinns_colab_spectrum():
    config = load_config()

    try:
        with open(config["paths"]["metrics"], "r") as f:
            metrics = json.load(f)
    except FileNotFoundError:
        print("Erro: O arquivo de métricas não foi encontrado. Rode o treino antes.")
        return

    E_dl = np.array(metrics["E_DL_Final"])
    E_base = np.array(metrics["E_MSG_Baseline"])
    error = np.array(metrics["Absolute_Error"])

    data = {}
    for i in range(len(E_dl)):
        data[f"E_{i+1}"] = [
            f"{E_base[i]:.10f}",
            f"{E_dl[i]:.10f}",
            f"{error[i]:.2e}",
        ]

    df = pd.DataFrame(
        data,
        index=["Baseline (MSG)", "DL (PINN)", "Erro Absoluto"],
    )
    print("\n=== Comparação Tabela 11 × PINN (15 níveis) ===")
    print(df)
    return df

"""**Rodar ou Executar o programa**"""

train_model_pinns_spectrum()
evaluate_pinns_colab_spectrum()
#plot_wavepacket_from_pinn()
plot_wavepacket_from_pinn(
    tau_values=[0, 1, 2, 3, 4, 5],
    num_superposed_states=3,
    path="results/figures/wavepacket_grid_V0-1.png"
)

"""Compactar o projeto para entrega"""

!zip -r dl_project_final.zip . -x "*__pycache__*" "*.ipynb_checkpoints*" "data/"